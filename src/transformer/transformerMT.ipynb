{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811bc228-f98d-491b-a382-c27dbf067733",
   "metadata": {},
   "source": [
    "# Sử dụng mô hình Transformer để giải quyết bài toán dịch máy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f170bd",
   "metadata": {},
   "source": [
    "## Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ded12a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import sacrebleu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f45d6a",
   "metadata": {},
   "source": [
    "## Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b322b249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device use: cpu\n"
     ]
    }
   ],
   "source": [
    "# Data path\n",
    "INPUT_DIR = \"../../data/IWSLT15/\"\n",
    "TRAIN_EN_PATH = f\"{INPUT_DIR}train.en.txt\"\n",
    "TRAIN_VI_PATH = f\"{INPUT_DIR}train.vi.txt\"\n",
    "\n",
    "TEST_EN_12_PATH = f\"{INPUT_DIR}tst2012.en.txt\"\n",
    "TEST_VI_12_PATH = f\"{INPUT_DIR}tst2012.vi.txt\"\n",
    "\n",
    "TEST_EN_13_PATH = f\"{INPUT_DIR}tst2013.en.txt\"\n",
    "TEST_VI_13_PATH = f\"{INPUT_DIR}tst2013.vi.txt\"\n",
    "\n",
    "SAVE_DIR = \"./checkpoints\"\n",
    "SPM_EN_PREFIX = os.path.join(SAVE_DIR, \"spm_en\")\n",
    "SPM_VI_PREFIX = os.path.join(SAVE_DIR, \"spm_vi\")\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 15000\n",
    "SEED = 42\n",
    "MAX_LEN=100\n",
    "BATCH_SIZE=64\n",
    "\n",
    "# Special tokens\n",
    "PAD_ID = 0\n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device use: {DEVICE}\")\n",
    "\n",
    "# Set random seeds\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814125e-4f16-425e-ad9c-4a44e2304796",
   "metadata": {},
   "source": [
    "## Data loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9eebcb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số cặp bị loại: 153\n",
      "Các dòng bị loại: [470, 8696, 9763, 10708, 21739, 26409, 29495, 38601, 39558, 41018, 48826, 50895, 51587, 54159, 56298, 57141, 57747, 58331, 66261, 68756] ...\n",
      "Số cặp bị loại: 0\n",
      "Các dòng bị loại: [] \n",
      "Training samples: 133164\n",
      "Test sample: 1553\n",
      "\n",
      "Example English sentence: In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .\n",
      "Example Vietnamese sentence: Trong 4 phút , chuyên gia hoá học khí quyển Rachel Pike giới thiệu sơ lược về những nỗ lực khoa học miệt mài đằng sau những tiêu đề táo bạo về biến đổi khí hậu , cùng với đoàn nghiên cứu của mình -- hàng ngàn người đã cống hiến cho dự án này -- một chuyến bay mạo hiểm qua rừng già để tìm kiếm thông tin về một phân tử then chốt .\n"
     ]
    }
   ],
   "source": [
    "def load_parallel(src_path, tgt_path):\n",
    "    \"\"\"\n",
    "    Đọc dữ liệu song ngữ và làm sạch:\n",
    "    - loại dòng rỗng\n",
    "    - loại dòng chỉ có '.'\n",
    "    - luôn giữ dữ liệu theo cặp (src[i] tương ứng tgt[i])\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_src = []\n",
    "    cleaned_tgt = []\n",
    "\n",
    "    bad_lines = []\n",
    "\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as fs, \\\n",
    "         open(tgt_path, \"r\", encoding=\"utf-8\") as ft:\n",
    "\n",
    "        for idx, (s, t) in enumerate(zip(fs, ft), start=1):\n",
    "            s = s.strip()\n",
    "            t = t.strip()\n",
    "\n",
    "            # if either side empty or is just \".\"\n",
    "            if (not s) or (not t) or s == \".\" or t == \".\":\n",
    "                bad_lines.append(idx)\n",
    "                continue\n",
    "\n",
    "            cleaned_src.append(s)\n",
    "            cleaned_tgt.append(t)\n",
    "\n",
    "    print(\"Số cặp bị loại:\", len(bad_lines))\n",
    "    print(\"Các dòng bị loại:\", bad_lines[:20], \"...\" if len(bad_lines) > 20 else \"\")\n",
    "\n",
    "    return cleaned_src, cleaned_tgt\n",
    "\n",
    "\n",
    "#Loading training and test data\n",
    "train_src, train_tgt = load_parallel(TRAIN_EN_PATH, TRAIN_VI_PATH)\n",
    "test_src, test_tgt = load_parallel(TEST_EN_12_PATH, TEST_VI_12_PATH)\n",
    "\n",
    "# test_src = readlines(TEST_EN_13_PATH)\n",
    "# test_tgt = readlines(TEST_VI_13_PATH)\n",
    "\n",
    "print(f\"Training samples: {len(train_src)}\")\n",
    "print(f\"Test sample: {len(test_src)}\")\n",
    "print(f\"\\nExample English sentence: {train_src[1]}\")\n",
    "print(f\"Example Vietnamese sentence: {train_tgt[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6eece9",
   "metadata": {},
   "source": [
    "## SentencePiece Tokenization\n",
    "Train BPE tokenizers for both English and Vietnamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "07d475c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spm(input_file, model_prefix, vocab_size=VOCAB_SIZE):\n",
    "    \"\"\"Train a SentencePiece BPE model\"\"\"\n",
    "    args = (\n",
    "        f\"--input={input_file} \"\n",
    "        f\"--model_prefix={model_prefix} \"\n",
    "        f\"--vocab_size={vocab_size} \"\n",
    "        \"--model_type=bpe \"\n",
    "        \"--character_coverage=1.0 \"\n",
    "        f\"--pad_id={PAD_ID} \"\n",
    "        f\"--unk_id={UNK_ID} \"\n",
    "        f\"--bos_id={BOS_ID} \"\n",
    "        f\"--eos_id={EOS_ID}\"\n",
    "    )\n",
    "\n",
    "    spm.SentencePieceTrainer.Train(args)\n",
    "    print(f\"Trained SentencePiece model: {model_prefix}.model\")\n",
    "\n",
    "def load_sp(model_path):\n",
    "    \"\"\"Load a trained SentencePiece model\"\"\"\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(model_path)\n",
    "    return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60d5357f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English vocab size: 15000\n",
      "Vietnamese vocab size: 15000\n",
      "\n",
      "Example tokenization:\n",
      "Original: Rachel Pike : The science behind a climate headline\n",
      "Token IDs: [10717, 299, 1267, 214, 155, 1116, 1724, 6, 2089, 10320]...\n"
     ]
    }
   ],
   "source": [
    "# Train English tokenizer\n",
    "tmp_en = os.path.join(SAVE_DIR, \"tmp_en.txt\")\n",
    "if not os.path.exists(SPM_EN_PREFIX + \".model\"):\n",
    "    with open(tmp_en, 'w', encoding='utf-8') as f:\n",
    "        for s in train_src:\n",
    "            f.write(s + \"\\n\")\n",
    "    train_spm(tmp_en, SPM_EN_PREFIX)\n",
    "\n",
    "# Train Vietnamese tokenizer\n",
    "tmp_vi = os.path.join(SAVE_DIR, \"tmp_vi.txt\")\n",
    "if not os.path.exists(SPM_VI_PREFIX + \".model\"):\n",
    "    with open(tmp_vi, 'w', encoding='utf-8') as f:\n",
    "        for s in train_tgt:\n",
    "            f.write(s + \"\\n\")\n",
    "    train_spm(tmp_vi, SPM_VI_PREFIX)\n",
    "\n",
    "# Load tokenizers\n",
    "sp_en = load_sp(SPM_EN_PREFIX + \".model\")\n",
    "sp_vi = load_sp(SPM_VI_PREFIX + \".model\")\n",
    "\n",
    "print(f\"\\nEnglish vocab size: {sp_en.GetPieceSize()}\")\n",
    "print(f\"Vietnamese vocab size: {sp_vi.GetPieceSize()}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_sent = train_src[0]\n",
    "tokens = sp_en.encode(test_sent)\n",
    "sent = sp_en.decode(tokens)\n",
    "print(f\"\\nExample tokenization:\")\n",
    "print(f\"Original: {sent}\")\n",
    "print(f\"Token IDs: {tokens[:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da360182",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d67b1",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b6885bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset Parallel \n",
    "    \"\"\"\n",
    "    def __init__(self, src, tgt, sp_src, sp_tgt, max_len=MAX_LEN):\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "        self.sp_src = sp_src\n",
    "        self.sp_tgt = sp_tgt\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src_ids = [BOS_ID] + self.sp_src.encode(self.src[index])[:self.max_len-2] + [EOS_ID]\n",
    "        tgt_ids = [BOS_ID] + self.sp_tgt.encode(self.tgt[index])[:self.max_len-2] + [EOS_ID]\n",
    "\n",
    "        return torch.tensor(src_ids), torch.tensor(tgt_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd721f",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1cb2d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giữ nguyên TranslationDataset của bạn\n",
    "# Tối ưu collate_fn một chút để an toàn hơn\n",
    "def collate_fn(batch):\n",
    "    src_list, tgt_list = zip(*batch)\n",
    "    \n",
    "    # Pad sequence bằng hàm có sẵn của Pytorch (nhanh hơn loop thủ công)\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    \n",
    "    # Batch first = True cho pad_sequence\n",
    "    src_pad = pad_sequence(src_list, batch_first=True, padding_value=PAD_ID)\n",
    "    tgt_pad = pad_sequence(tgt_list, batch_first=True, padding_value=PAD_ID)\n",
    "    \n",
    "    # Tách tgt_in (bỏ token cuối) và tgt_out (bỏ token đầu)\n",
    "    # Lưu ý: slicing tensor vẫn giữ nguyên shape batch\n",
    "    tgt_in = tgt_pad[:, :-1]\n",
    "    tgt_out = tgt_pad[:, 1:]\n",
    "    \n",
    "    return src_pad, tgt_in, tgt_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc8a12b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples src: 133164\n",
      "Total samples src: 133164\n",
      "Training size: 119847\n",
      "Validation size: 13317\n",
      "\n",
      "Training batches: 1873\n",
      "Validation batches: 209\n",
      "[1, 134, 87, 101, 10, 1161, 276, 326, 748, 211, 127, 240, 1324, 12, 2]\n",
      "[1, 383, 26, 38, 1481, 308, 167, 531, 712, 14, 2]\n",
      "So this was the model which actually came out -- very amazing .\n",
      "Đây là một sơ đồ rất tuyệt vời ,\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset and dataloader\n",
    "# Split data: 90% train, 10% validation\n",
    "\n",
    "total_samples_src = len(train_src)\n",
    "total_samples_tgt = len(train_tgt)\n",
    "train_size = int(0.9 * total_samples_src)\n",
    "\n",
    "train_src_split = train_src[:train_size]\n",
    "train_tgt_split = train_tgt[:train_size]\n",
    "val_src = train_src[train_size:]\n",
    "val_tgt = train_tgt[train_size:]\n",
    "\n",
    "print(f\"Total samples src: {total_samples_src}\")\n",
    "print(f\"Total samples src: {total_samples_tgt}\")\n",
    "print(f\"Training size: {len(train_tgt_split)}\")\n",
    "print(f\"Validation size: {len(val_src)}\")\n",
    "\n",
    "train_dataset = TranslationDataset(src=train_src_split, tgt=train_tgt_split, sp_src=sp_en, sp_tgt=sp_vi)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "val_dataset = TranslationDataset(src=val_src, tgt=val_tgt, sp_src=sp_en, sp_tgt=sp_vi)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "print(f\"\\nTraining batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "src_tk = val_dataset[1][0].tolist()\n",
    "tgt_tk = val_dataset[1][1].tolist()\n",
    "\n",
    "print(src_tk)\n",
    "print(tgt_tk)\n",
    "print(sp_en.decode(src_tk))\n",
    "print(sp_vi.decode(tgt_tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b99aeb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: tensor([[    1, 11950,   619,  ...,     0,     0,     0],\n",
      "        [    1,   502,    65,  ...,     0,     0,     0],\n",
      "        [    1,   502,    65,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    1,   344,     8,  ...,     0,     0,     0],\n",
      "        [    1,   502,   109,  ...,     0,     0,     0],\n",
      "        [    1,    83,   311,  ...,     0,     0,     0]])\n",
      "\n",
      "TGT_IN: tensor([[   1, 1017,  128,  ...,    0,    0,    0],\n",
      "        [   1,  482,  167,  ...,    0,    0,    0],\n",
      "        [   1,  482,  447,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   1,  522,   70,  ...,    0,    0,    0],\n",
      "        [   1,  482,  536,  ...,    0,    0,    0],\n",
      "        [   1,   93,  424,  ...,    0,    0,    0]])\n",
      "\n",
      "TGT_OUT: tensor([[1017,  128,  486,  ...,    0,    0,    0],\n",
      "        [ 482,  167,  172,  ...,    0,    0,    0],\n",
      "        [ 482,  447, 1058,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 522,   70,  486,  ...,    0,    0,    0],\n",
      "        [ 482,  536,   38,  ...,    0,    0,    0],\n",
      "        [  93,  424,   39,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "for src, tgt_in, tgt_out in train_loader:\n",
    "    print(f\"SRC: {src}\")\n",
    "    print(f\"\\nTGT_IN: {tgt_in}\")\n",
    "    print(f\"\\nTGT_OUT: {tgt_out}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eea2a9",
   "metadata": {},
   "source": [
    "## TransformerMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8e47227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMT(nn.Module):\n",
    "    def __init__(self, sp_src_size, sp_tgt_size, d_model=512, nhead=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.d_model = d_model\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.src_tok_emb = nn.Embedding(sp_src_size, d_model, padding_idx=pad_idx)\n",
    "        self.tgt_tok_emb = nn.Embedding(sp_tgt_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # QUAN TRỌNG: batch_first=True\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers,\n",
    "                                          dim_feedforward=dim_feedforward,\n",
    "                                          dropout=dropout,\n",
    "                                          batch_first=True) \n",
    "                                          \n",
    "        self.generator = nn.Linear(d_model, sp_tgt_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.src_tok_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.tgt_tok_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.generator.bias.data.zero_()\n",
    "        self.generator.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def encode(self, src, src_key_padding_mask=None):\n",
    "        # src: [batch, seq_len] -> embedding: [batch, seq_len, d_model]\n",
    "        src_emb = self.src_tok_emb(src) * math.sqrt(self.d_model)\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        # Không cần transpose nữa\n",
    "        return self.transformer.encoder(src_emb, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        tgt_emb = self.tgt_tok_emb(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        return self.transformer.decoder(tgt_emb, memory,\n",
    "                                          tgt_mask=tgt_mask,\n",
    "                                          tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                          memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "    def forward(self, src, tgt_in, src_key_padding_mask=None, tgt_key_padding_mask=None, tgt_mask=None):\n",
    "        src_emb = self.src_tok_emb(src) * math.sqrt(self.d_model)\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        \n",
    "        tgt_emb = self.tgt_tok_emb(tgt_in) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "\n",
    "        # Transformer với batch_first=True tự động xử lý\n",
    "        outs = self.transformer(src_emb, tgt_emb, \n",
    "                                tgt_mask=tgt_mask, \n",
    "                                src_key_padding_mask=src_key_padding_mask,\n",
    "                                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                memory_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        return self.generator(outs)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # compute positional encodings once\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 1:\n",
    "            # odd dims: last column will be zero for cos\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones((sz, sz), device='cpu') * float('-inf'), diagonal=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf92ec",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c40d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerMT(sp_src_size=VOCAB_SIZE, sp_tgt_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708053ec",
   "metadata": {},
   "source": [
    "### Train/Val epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44d400de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, src_tokens, sp_src, sp_tgt, device, max_len=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode src\n",
    "        src_ids = torch.tensor([sp_src.encode(src_tokens)], dtype=torch.long).to(device)\n",
    "        src_mask = (src_ids == PAD_ID)\n",
    "        memory = model.encode(src_ids, src_key_padding_mask=src_mask)\n",
    "        \n",
    "        # Bắt đầu với BOS\n",
    "        ys = torch.tensor([[BOS_ID]], dtype=torch.long).to(device)\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            tgt_mask = generate_square_subsequent_mask(ys.size(1)).to(device)\n",
    "            \n",
    "            # Decode (chú ý batch_first logic)\n",
    "            out = model.decode(ys, memory, tgt_mask=tgt_mask, memory_key_padding_mask=src_mask)\n",
    "            \n",
    "            # Generator\n",
    "            prob = model.generator(out[:, -1]) # Chỉ lấy token cuối cùng\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            next_word = next_word.item()\n",
    "\n",
    "            ys = torch.cat([ys, torch.tensor([[next_word]], device=device)], dim=1)\n",
    "            if next_word == EOS_ID:\n",
    "                break\n",
    "        \n",
    "        # Decode token IDs sang text\n",
    "        final_ids = ys.squeeze(0).tolist()\n",
    "        # Remove BOS/EOS if needed for cleaner output\n",
    "        if final_ids[0] == BOS_ID: final_ids = final_ids[1:]\n",
    "        if final_ids[-1] == EOS_ID: final_ids = final_ids[:-1]\n",
    "            \n",
    "        return sp_tgt.decode(final_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f85ccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1, 10717,   299,  1267,   214,   155,  1116,  1724,     6,  2089,\n",
      "        10320,     2])\n",
      "Rachel Pike : The science behind a climate headline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'itageitageitageitageitageitageitageitageitageitage'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_token = train_dataset[0][0]\n",
    "sample_sent = sp_en.decode(sample_token.tolist())\n",
    "print(sample_token)\n",
    "print(sample_sent)\n",
    "\n",
    "translate_sentence(model=model, src_tokens=sample_sent, sp_src=sp_en, sp_tgt=sp_vi, device=DEVICE, max_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d699121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Training/Validation loops\n",
    "# ----------------------------\n",
    "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    p_bar = tqdm(dataloader, desc=\"Train\", leave=False)\n",
    "    for src, tgt_in, tgt_out in p_bar:\n",
    "        src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "\n",
    "        # Masks\n",
    "        src_key_padding_mask = (src == PAD_ID)\n",
    "        tgt_key_padding_mask = (tgt_in == PAD_ID)\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_in.size(1)).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed Precision Training\n",
    "        with autocast(): # Tự động chuyển float32 -> float16 khi cần\n",
    "            output = model(src, tgt_in, \n",
    "                           src_key_padding_mask=src_key_padding_mask, \n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask, \n",
    "                           tgt_mask=tgt_mask)\n",
    "            \n",
    "            # Reshape để tính loss\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), tgt_out.reshape(-1))\n",
    "\n",
    "        # Backward với scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # QUAN TRỌNG: Step scheduler tại mỗi batch\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def val_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for src_batch, tgt_in_batch, tgt_out_batch in dataloader:\n",
    "          src = src_batch.to(device)\n",
    "          tgt_in = tgt_in_batch.to(device)\n",
    "          tgt_out = tgt_out_batch.to(device)\n",
    "\n",
    "          src_key_padding_mask = (src == 0)\n",
    "          tgt_key_padding_mask = (tgt_in == 0)\n",
    "\n",
    "          tgt_mask = generate_square_subsequent_mask(sz=tgt_in.size(1)).to(device)\n",
    "          output = model(src, tgt_in,\n",
    "                        src_key_padding_mask=src_key_padding_mask,\n",
    "                        tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                        memory_key_padding_mask=src_key_padding_mask,\n",
    "                        tgt_mask=tgt_mask)\n",
    "          # output shape = (batch, tgt_len, vocab)\n",
    "          loss = criterion(output.view(-1, output.size(-1)), tgt_out.view(-1))\n",
    "          total_loss += loss.item() * src.size(0)\n",
    "      return total_loss/len(dataloader.dataset)\n",
    "    \n",
    "@torch.no_grad()\n",
    "def evaluate_bleu(model, dataloader, sp_src, sp_tgt, device):\n",
    "    \"\"\"Evaluate model using SacreBLEU metric.\"\"\"\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "\n",
    "    for src, tgt_in, tgt_out in pbar:\n",
    "        src = src.to(device)\n",
    "        tgt_in = tgt_in.to(device)\n",
    "\n",
    "        # ===== Decode source =====\n",
    "        src_ids = src[0].tolist()         # lấy 1 câu trong batch\n",
    "        src_ids = [x for x in src_ids if x != 0]   # remove padding\n",
    "        src_text = sp_src.decode(src_ids)\n",
    "\n",
    "        # print(\"=\"*80)\n",
    "        # print(f\"SRC_sentence: {src_text}\")\n",
    "        # ===== Translate =====\n",
    "        pred_text = translate_sentence(\n",
    "            model,\n",
    "            src_text,\n",
    "            sp_src=sp_src,\n",
    "            sp_tgt=sp_tgt,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # print(f\"PRED_sentence: {pred_text}\")\n",
    "\n",
    "        # ===== Decode reference =====\n",
    "        ref_ids = tgt_in[0].tolist()\n",
    "        ref_ids = [x for x in ref_ids if x != 0]\n",
    "        ref_text = sp_tgt.decode(ref_ids)\n",
    "\n",
    "        # print(f\"REF_sentence: {ref_text}\")\n",
    "\n",
    "        # print(\"=\"*80)\n",
    "        preds.append(pred_text)\n",
    "        refs.append(ref_text)\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(preds, [refs])\n",
    "    return bleu.score\n",
    "# evaluate_bleu(model=model, dataloader=val_loader, sp_src=sp_en, sp_tgt=sp_vi, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612f0bf",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f51e582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Early Stopping\n",
    "# ----------------------------\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                return True\n",
    "            return False\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeef9375",
   "metadata": {},
   "source": [
    "### Save/load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aba0b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Save / Load\n",
    "# ----------------------------\n",
    "def save_checkpoint(path, model, optimizer, scheduler, epoch, best=False):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optim_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    if best:\n",
    "        best_path = os.path.splitext(path)[0] + \".best.pt\"\n",
    "        torch.save(state, best_path)\n",
    "\n",
    "def load_checkpoint(path, model, optimizer=None, scheduler=None, map_location='cpu'):\n",
    "    ckpt = torch.load(path, map_location=map_location)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    if optimizer and ckpt.get('optim_state_dict'):\n",
    "        optimizer.load_state_dict(ckpt['optim_state_dict'])\n",
    "    if scheduler and ckpt.get('scheduler_state_dict'):\n",
    "        scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
    "    return ckpt.get('epoch', 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b10a12",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a40de013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    Công thức Noam learning rate schedule.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5)))\n",
    "\n",
    "# Khi khởi tạo trong main:\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9)\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: rate(step, 512, 1.0, 4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e69a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/62/f6sjw4bd0td2_b5c63kfyh0c0000gn/T/ipykernel_92415/4075438551.py:22: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() # Cho mixed precision\n",
      "/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">transformer_mt_run</strong> at: <a href='https://wandb.ai/ngocbaotrinhtuan-vietnam-national-university-hanoi/mt-project/runs/dloprrt1' target=\"_blank\">https://wandb.ai/ngocbaotrinhtuan-vietnam-national-university-hanoi/mt-project/runs/dloprrt1</a><br> View project at: <a href='https://wandb.ai/ngocbaotrinhtuan-vietnam-national-university-hanoi/mt-project' target=\"_blank\">https://wandb.ai/ngocbaotrinhtuan-vietnam-national-university-hanoi/mt-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251119_105735-dloprrt1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ngocbao/Documents/Document/Season5/NLP/code/Machine Translation/src/transformer/wandb/run-20251119_105756-yrhsyae3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ngocbaotrinhtuan-vietnam-national-university-hanoi/mt-project/runs/yrhsyae3' target=\"_blank\">transformer_mt_run</a></strong> to <a href='https://wandb.ai/ngocbaotrinhtuan-vietnam-national-university-hanoi/mt-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ngocbaotrinhtuan-vietnam-national-university-hanoi/mt-project' target=\"_blank\">https://wandb.ai/ngocbaotrinhtuan-vietnam-national-university-hanoi/mt-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ngocbaotrinhtuan-vietnam-national-university-hanoi/mt-project/runs/yrhsyae3' target=\"_blank\">https://wandb.ai/ngocbaotrinhtuan-vietnam-national-university-hanoi/mt-project/runs/yrhsyae3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/1873 [00:00<?, ?it/s]/var/folders/62/f6sjw4bd0td2_b5c63kfyh0c0000gn/T/ipykernel_92415/2332817186.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(): # Tự động chuyển float32 -> float16 khi cần\n",
      "/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "socket.send() raised exception.                                                   \n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Config\n",
    "# ============================\n",
    "config = {\n",
    "    \"lr\": 1e-7,\n",
    "    \"epochs\": 50,\n",
    "    \"patience\": 5,\n",
    "    \"run_name\": \"transformer_mt_run\",\n",
    "    \"save_dir\": \"./checkpoints\",\n",
    "    \"use_wandb\": True,\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Model setup\n",
    "# ============================\n",
    "model = TransformerMT(VOCAB_SIZE, VOCAB_SIZE)\n",
    "model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-7, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: rate(step, 512, 1.0, 4000))\n",
    "scaler = GradScaler() # Cho mixed precision\n",
    "# ============================\n",
    "# Wandb (optional)\n",
    "# ============================\n",
    "use_wandb = config[\"use_wandb\"]\n",
    "\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    wandb.init(project=\"mt-project\", name=config[\"run_name\"], config=config)\n",
    "    wandb.watch(model, log=\"gradients\", log_freq=100)\n",
    "\n",
    "# ============================\n",
    "# Early stopping\n",
    "# ============================\n",
    "early_stopper = EarlyStopping(patience=config[\"patience\"])\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "# ============================\n",
    "# Training Loop\n",
    "# ============================\n",
    "bleu = 0.0\n",
    "for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    start = time.time()\n",
    "\n",
    "    # Train\n",
    "    train_loss = train_epoch(model=model, dataloader=train_loader, criterion=criterion,\n",
    "                            optimizer=optimizer, device=DEVICE, scaler=scaler, scheduler=scheduler)\n",
    "\n",
    "    # Eval loss + BLEU\n",
    "    val_loss = val_epoch(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    epoch_time = time.time() - start\n",
    "\n",
    "\n",
    "    # Print\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Time: {epoch_time:.1f}s\"\n",
    "    )\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        bleu = evaluate_bleu(model, val_loader, sp_en, sp_vi, DEVICE)\n",
    "        print(f\"Epoch {epoch} | BLEU: {bleu}\")\n",
    "\n",
    "    # Wandb\n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"bleu_score\": bleu,\n",
    "            \"epoch\": epoch,\n",
    "            \"time\": epoch_time\n",
    "        })\n",
    "\n",
    "    # Save last\n",
    "    latest_path = os.path.join(config[\"save_dir\"], f\"{config['run_name']}.epoch{epoch}.pt\")\n",
    "    save_checkpoint(latest_path, model, optimizer, scheduler, epoch, best=False)\n",
    "\n",
    "    # Save best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_path = os.path.join(config[\"save_dir\"], f\"{config['run_name']}.best.pt\")\n",
    "        save_checkpoint(best_path, model, optimizer, scheduler, epoch, best=True)\n",
    "        print(f\"New best model saved to {best_path}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if early_stopper.step(val_loss):\n",
    "        print(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Save final model\n",
    "# ============================\n",
    "final_path = os.path.join(config[\"save_dir\"], f\"{config['run_name']}.final.pt\")\n",
    "save_checkpoint(final_path, model, optimizer, scheduler, epoch, best=False)\n",
    "print(\"Training finished. Final model saved to\", final_path)\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.finish()\n",
    "\n",
    "# ============================\n",
    "# Quick inference demo\n",
    "# ============================\n",
    "print(\"\\n=== PREDICTION DEMO ===\")\n",
    "model.eval() # Đảm bảo model ở chế độ eval\n",
    "for _ in range(5): # Demo 5 câu thôi cho gọn\n",
    "    if len(val_src) > 0:\n",
    "        i = random.randrange(len(val_src))\n",
    "        src_example = val_src[i]\n",
    "        tgt_example = val_tgt[i]\n",
    "\n",
    "        print(f\"Source: {src_example}\")\n",
    "        print(f\"Target: {tgt_example}\")\n",
    "\n",
    "        # Sửa tên tham số cho đúng định nghĩa hàm\n",
    "        pred = translate_sentence(\n",
    "            model, \n",
    "            src_example, \n",
    "            sp_src=sp_en,  # Đúng tên tham số là sp_src\n",
    "            sp_tgt=sp_vi,  # Đúng tên tham số là sp_tgt\n",
    "            device=DEVICE\n",
    "        )\n",
    "        print(f\"Pred  : {pred}\")\n",
    "        print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
